{
 "cells": [
  {
   "cell_type": "code",
   "id": "dac014cb",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = kagglehub.dataset_download(\"suchintikasarkar/sentiment-analysis-for-mental-health\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c78fecdb",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "file_name = \"Combined Data.csv\"\n",
    "file_path = os.path.join(path, file_name)\n",
    "\n",
    "print(f\"Attempting to load data from: {file_path}\")\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "    df.dropna(subset=['statement'], inplace=True)\n",
    "except Exception:\n",
    "    print(\"Could not load data from path, please ensure file path is correct.\")\n",
    "\n",
    "# --- 2. Label Encoding ---\n",
    "le = LabelEncoder()\n",
    "df['status_encoded'] = le.fit_transform(df['status'])\n",
    "\n",
    "# Get the number of unique classes\n",
    "NUM_LABELS = len(le.classes_)\n",
    "\n",
    "# Map for decoding results later\n",
    "label_map = {i: label for i, label in enumerate(le.classes_)}\n",
    "print(f\"Number of classes (NUM_LABELS): {NUM_LABELS}\")\n",
    "\n",
    "# --- 3. Data Splitting ---\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['status_encoded']\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Validation samples: {len(val_df)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "896149da",
   "metadata": {},
   "source": [
    "# --- 4. Custom Dataset Class for BERT ---\n",
    "\n",
    "# Initialize BERT Tokenizer\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "MAX_LEN = 128  # Set maximum sequence length, adjust based on data analysis\n",
    "\n",
    "\n",
    "class MentalHealthDataset(Dataset):\n",
    "    \"\"\"A custom PyTorch Dataset for loading and tokenizing the text data.\"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        label = self.labels[item]\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',  # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0361ee9",
   "metadata": {},
   "source": [
    "# --- 5. Model Initialization ---\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = MentalHealthDataset(\n",
    "    train_df['statement'],\n",
    "    train_df['status_encoded'],\n",
    "    tokenizer,\n",
    "    MAX_LEN\n",
    ")\n",
    "\n",
    "val_dataset = MentalHealthDataset(\n",
    "    val_df['statement'],\n",
    "    val_df['status_encoded'],\n",
    "    tokenizer,\n",
    "    MAX_LEN\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16  # Adjust based on GPU memory\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load BERT with a classification layer configured for 7 labels\n",
    "# Note: Ensure NUM_LABELS is correctly defined from the previous steps\n",
    "try:\n",
    "    if 'NUM_LABELS' not in locals():\n",
    "        # Fallback in case previous cells were not executed in the same kernel session\n",
    "        NUM_LABELS = df['status_encoded'].nunique()\n",
    "        PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "except NameError:\n",
    "    print(\"Error: 'df' or 'NUM_LABELS' is not defined. Please run Part 1 and Part 2 cells first.\")\n",
    "    # Exit or define placeholders\n",
    "    exit()\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME,\n",
    "    num_labels=NUM_LABELS\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# --- 6. Optimizer and Parameters ---\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "print(\"\\nOptimizer AdamW initialized successfully.\")\n",
    "\n",
    "# --- 7. Training Loop Function ---\n",
    "def train_epoch(model, data_loader, optimizer, device, n_examples):\n",
    "    \"\"\"Performs one epoch of training.\"\"\"\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    for d in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels = d[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "# --- 8. Evaluation Function ---\n",
    "def eval_model(model, data_loader, device):\n",
    "    \"\"\"Evaluates the model on the validation set.\"\"\"\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        # Use tqdm for progress bar\n",
    "        for d in tqdm(data_loader, desc=\"Validation\"):\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / len(all_labels), np.mean(losses), all_labels, all_preds\n",
    "\n",
    "# --- 9. Main Training Loop Execution and Tracking ---\n",
    "\n",
    "print(\"\\n--- Starting BERT Fine-tuning ---\")\n",
    "best_accuracy = 0\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    # Training\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        len(train_df)\n",
    "    )\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc.item())\n",
    "\n",
    "    print(f\"Train loss {train_loss:.4f} | Accuracy {train_acc:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_acc, val_loss, Y_true, Y_pred = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        device\n",
    "    )\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc.item())\n",
    "\n",
    "    print(f\"Val loss   {val_loss:.4f} | Accuracy {val_acc:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_bert_model.bin')\n",
    "        best_accuracy = val_acc\n",
    "        print(f\"Model saved! New best accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# --- 10. Visualization of Training History ---\n",
    "\n",
    "def plot_history(history):\n",
    "    \"\"\"Plots training and validation loss and accuracy over epochs.\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n--- Plotting Training History ---\")\n",
    "plot_history(history)\n",
    "\n",
    "# --- 11. Final Evaluation ---\n",
    "\n",
    "# Load the best model weights for final evaluation\n",
    "model.load_state_dict(torch.load('best_bert_model.bin'))\n",
    "print(\"\\n--- Final Evaluation on Validation Set (Using Best Model) ---\")\n",
    "\n",
    "# Run evaluation with the best model to get the final predictions\n",
    "_, _, Y_true, Y_pred = eval_model(\n",
    "    model,\n",
    "    val_data_loader,\n",
    "    device\n",
    ")\n",
    "\n",
    "# Print detailed classification report\n",
    "# Note: Ensure the 'label_map' and 'target_names_list' are correctly defined from Part 1\n",
    "target_names_list = [label_map[i] for i in range(NUM_LABELS)]\n",
    "print(\"\\n--- Classification Report (Best Model) ---\")\n",
    "print(classification_report(Y_true, Y_pred, target_names=target_names_list))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hello_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
