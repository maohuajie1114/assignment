{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4cc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\hello_pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/suchintikasarkar/sentiment-analysis-for-mental-health?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11.1M/11.1M [00:01<00:00, 6.45MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\25769\\.cache\\kagglehub\\datasets\\suchintikasarkar\\sentiment-analysis-for-mental-health\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"suchintikasarkar/sentiment-analysis-for-mental-health\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175df83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from: C:\\Users\\25769\\.cache\\kagglehub\\datasets\\suchintikasarkar\\sentiment-analysis-for-mental-health\\versions\\1\\Combined Data.csv\n",
      "\n",
      "--- 1. DataFrame Head (First 5 Rows) ---\n",
      "   Unnamed: 0                                          statement   status\n",
      "0           0                                         oh my gosh  Anxiety\n",
      "1           1  trouble sleeping, confused mind, restless hear...  Anxiety\n",
      "2           2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n",
      "3           3  I've shifted my focus to something else but I'...  Anxiety\n",
      "4           4  I'm restless and restless, it's been a month n...  Anxiety\n",
      "\n",
      "--- 2. DataFrame Information (Columns, Non-null counts, Dtypes) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53043 entries, 0 to 53042\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  53043 non-null  int64 \n",
      " 1   statement   52681 non-null  object\n",
      " 2   status      53043 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.2+ MB\n",
      "None\n",
      "\n",
      "--- 3. Dataset Shape (Rows, Columns) ---\n",
      "(53043, 3)\n",
      "\n",
      "--- 4. Target Variable Distribution (Sentiment/Label) ---\n",
      "Could not find a 'label' column. Please check the actual column names from .head()\n",
      "\n",
      "Object (Text) Column Unique Values Check:\n",
      "Column 'status' value counts:\n",
      "status\n",
      "Normal                  16351\n",
      "Depression              15404\n",
      "Suicidal                10653\n",
      "Anxiety                  3888\n",
      "Bipolar                  2877\n",
      "Stress                   2669\n",
      "Personality disorder     1201\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "--- 5. Check for Missing Values ---\n",
      "Unnamed: 0      0\n",
      "statement     362\n",
      "status          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming the path printed from the kagglehub download is correct\n",
    "# We need to find the specific CSV file within that downloaded directory.\n",
    "# Based on the dataset name, the main data file is likely named 'mental_health_sentiment.csv'\n",
    "# If the path structure is complex, you may need to adjust the file name/path.\n",
    "\n",
    "# Replace 'path' with the actual path variable from your kagglehub download output\n",
    "dataset_path = path # Example path, replace with actual output\n",
    "\n",
    "file_name = \"Combined Data.csv\"\n",
    "file_path = os.path.join(dataset_path, file_name)\n",
    "\n",
    "print(f\"Attempting to load data from: {file_path}\")\n",
    "\n",
    "try:\n",
    "    # Load the dataset into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    print(\"\\n--- 1. DataFrame Head (First 5 Rows) ---\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\n--- 2. DataFrame Information (Columns, Non-null counts, Dtypes) ---\")\n",
    "    print(df.info())\n",
    "\n",
    "    print(\"\\n--- 3. Dataset Shape (Rows, Columns) ---\")\n",
    "    print(df.shape)\n",
    "\n",
    "    print(\"\\n--- 4. Target Variable Distribution (Sentiment/Label) ---\")\n",
    "    # Assuming the target column is named 'label' or 'sentiment'\n",
    "    # We will try 'label' based on similar datasets, you might need to adjust this.\n",
    "    try:\n",
    "        target_column = 'label'  # Adjust if the actual column name is different\n",
    "        print(df[target_column].value_counts())\n",
    "        print(f\"\\nTarget Variable Unique Values: {df[target_column].nunique()}\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(\"Could not find a 'label' column. Please check the actual column names from .head()\")\n",
    "        # Fallback: check all unique values in all object columns\n",
    "        print(\"\\nObject (Text) Column Unique Values Check:\")\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "             if df[col].nunique() < 20: # Likely a category column\n",
    "                 print(f\"Column '{col}' value counts:\\n{df[col].value_counts()}\\n\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. Check for Missing Values ---\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: File not found at {file_path}. Please check the downloaded path and file name.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during data loading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ad3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after handling missing values: (52681, 2)\n",
      "\n",
      "--- Label Encoding Mapping ---\n",
      "Anxiety: 0\n",
      "Bipolar: 1\n",
      "Depression: 2\n",
      "Normal: 3\n",
      "Personality disorder: 4\n",
      "Stress: 5\n",
      "Suicidal: 6\n",
      "\n",
      "--- Loading GPT-2 Model and Tokenizer (This may take a moment) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\hello_pytorch\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\25769\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Feature Matrix X shape: (52681, 768)\n",
      "Generated Label Vector Y shape: (52681,)\n",
      "\n",
      "Training on 42144 samples, Testing on 10537 samples.\n",
      "\n",
      "--- Training Logistic Regression Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\hello_pytorch\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results (Logistic Regression with GPT-2 Features) ---\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Anxiety       0.75      0.71      0.73       768\n",
      "             Bipolar       0.76      0.71      0.73       556\n",
      "          Depression       0.68      0.73      0.70      3081\n",
      "              Normal       0.88      0.94      0.91      3269\n",
      "Personality disorder       0.73      0.50      0.60       215\n",
      "              Stress       0.55      0.43      0.48       517\n",
      "            Suicidal       0.67      0.61      0.64      2131\n",
      "\n",
      "            accuracy                           0.75     10537\n",
      "           macro avg       0.72      0.66      0.68     10537\n",
      "        weighted avg       0.74      0.75      0.74     10537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\hello_pytorch\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 1. Load Data (assuming you have the 'df' DataFrame from your output) ---\n",
    "\n",
    "# Re-creating the DataFrame structure based on your output for demonstration\n",
    "# In your actual environment, you'll use the existing 'df'\n",
    "dataset_path = \"C:\\\\Users\\\\25769\\\\.cache\\\\kagglehub\\\\datasets\\\\suchintikasarkar\\\\sentiment-analysis-for-mental-health\\\\versions\\\\1\"\n",
    "file_path = os.path.join(dataset_path, \"Combined Data.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the irrelevant index column\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# --- 2. Data Cleaning and Preprocessing ---\n",
    "\n",
    "# Handle missing values by dropping rows where 'statement' is NaN\n",
    "# Since only 362 out of 53043 are missing, dropping them is generally safe.\n",
    "df.dropna(subset=['statement'], inplace=True)\n",
    "print(f\"Data shape after handling missing values: {df.shape}\")\n",
    "\n",
    "# Convert all text to string (ensures robustness)\n",
    "df['statement'] = df['statement'].astype(str)\n",
    "\n",
    "# --- 3. Label Encoding for the Target Variable (status) ---\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Fit and transform the 'status' column\n",
    "df['status_encoded'] = le.fit_transform(df['status'])\n",
    "\n",
    "# Display the mapping\n",
    "print(\"\\n--- Label Encoding Mapping ---\")\n",
    "for i, label in enumerate(le.classes_):\n",
    "    print(f\"{label}: {i}\")\n",
    "\n",
    "# --- 4. GPT-2 Feature Extraction Function ---\n",
    "\n",
    "def get_gpt2_embeddings(texts, model, tokenizer, batch_size=32):\n",
    "    \"\"\"Generates sentence embeddings using GPT-2 by mean pooling.\"\"\"\n",
    "\n",
    "    # Set pad token for GPT-2 (it doesn't have one by default)\n",
    "    # Using the EOS token as PAD token is a common workaround for feature extraction\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Place model in evaluation mode and move to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Process texts in batches to manage memory\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        # Padding='longest' for dynamic padding, Truncation=True to prevent sequence overflow\n",
    "        # return_tensors='pt' returns PyTorch tensors\n",
    "        encoded_input = tokenizer(\n",
    "            batch_texts.tolist(), \n",
    "            padding='longest', \n",
    "            truncation=True, \n",
    "            max_length=128, # Choose a max length based on data, 128 is common\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move tensors to the appropriate device\n",
    "        input_ids = encoded_input['input_ids'].to(device)\n",
    "        attention_mask = encoded_input['attention_mask'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get the model output\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # The last hidden state contains the token embeddings\n",
    "            embeddings = output.last_hidden_state # Shape: (Batch Size, Sequence Length, Hidden Dimension)\n",
    "\n",
    "        # --- Mean Pooling to get Sentence Vector ---\n",
    "        # Multiply embeddings by the attention mask to zero out padding tokens\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "        masked_embeddings = embeddings * mask_expanded\n",
    "        \n",
    "        # Sum all non-padding tokens\n",
    "        summed_embeddings = torch.sum(masked_embeddings, 1)\n",
    "        \n",
    "        # Calculate the actual number of tokens (non-padding)\n",
    "        summed_mask = torch.clamp(attention_mask.sum(1), min=1e-9) # Avoid division by zero\n",
    "        \n",
    "        # Divide sum by count to get the mean (mean pooling)\n",
    "        mean_pooled_embeddings = (summed_embeddings / summed_mask.unsqueeze(-1)).cpu().numpy()\n",
    "        \n",
    "        all_embeddings.append(mean_pooled_embeddings)\n",
    "\n",
    "    return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "\n",
    "# Initialize GPT-2 components\n",
    "print(\"\\n--- Loading GPT-2 Model and Tokenizer (This may take a moment) ---\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Generate the feature vectors (X)\n",
    "X_features = get_gpt2_embeddings(df['statement'], model, tokenizer, batch_size=64)\n",
    "Y_labels = df['status_encoded'].values\n",
    "\n",
    "print(f\"\\nGenerated Feature Matrix X shape: {X_features.shape}\")\n",
    "print(f\"Generated Label Vector Y shape: {Y_labels.shape}\")\n",
    "\n",
    "# --- 5. Data Splitting and Logistic Regression ---\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_features, Y_labels, test_size=0.2, random_state=42, stratify=Y_labels\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining on {X_train.shape[0]} samples, Testing on {X_test.shape[0]} samples.\")\n",
    "\n",
    "# Initialize and train Logistic Regression model\n",
    "print(\"\\n--- Training Logistic Regression Model ---\")\n",
    "# Use a high max_iter and appropriate solver for large datasets/features\n",
    "lr_model = LogisticRegression(max_iter=500, solver='sag', multi_class='multinomial', random_state=42)\n",
    "lr_model.fit(X_train, Y_train)\n",
    "\n",
    "# --- 6. Model Evaluation ---\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred = lr_model.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Evaluation Results (Logistic Regression with GPT-2 Features) ---\")\n",
    "print(classification_report(Y_test, Y_pred, target_names=le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hello_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
