{
 "cells": [
  {
   "cell_type": "code",
   "id": "85c4cc90",
   "metadata": {},
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import os\n",
    "\n",
    "path = kagglehub.dataset_download(\"suchintikasarkar/sentiment-analysis-for-mental-health\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "175df83c",
   "metadata": {},
   "source": [
    "# --- 1. Load Data ---\n",
    "\n",
    "file_name = \"Combined Data.csv\"\n",
    "file_path = os.path.join(path, file_name)\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"Attempting to load data from: {file_path}\")\n",
    "\n",
    "try:\n",
    "    # Load the dataset into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    print(\"\\n--- 1. DataFrame Head (First 5 Rows) ---\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\n--- 2. DataFrame Information (Columns, Non-null counts, Dtypes) ---\")\n",
    "    print(df.info())\n",
    "\n",
    "    print(\"\\n--- 3. Dataset Shape (Rows, Columns) ---\")\n",
    "    print(df.shape)\n",
    "\n",
    "    print(\"\\n--- 4. Target Variable Distribution (Sentiment/Label) ---\")\n",
    "    try:\n",
    "        target_column = 'label'\n",
    "        print(df[target_column].value_counts())\n",
    "        print(f\"\\nTarget Variable Unique Values: {df[target_column].nunique()}\")\n",
    "    except KeyError:\n",
    "        print(\"Could not find a 'label' column. Please check the actual column names from .head()\")\n",
    "        # Fallback: check all unique values in all object columns\n",
    "        print(\"\\nObject (Text) Column Unique Values Check:\")\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            if df[col].nunique() < 20:\n",
    "                print(f\"Column '{col}' value counts:\\n{df[col].value_counts()}\\n\")\n",
    "\n",
    "    print(\"\\n--- 5. Check for Missing Values ---\")\n",
    "    print(df.isnull().sum())\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: File not found at {file_path}. Please check the downloaded path and file name.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during data loading: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "724ad3b2",
   "metadata": {},
   "source": [
    "# --- 2. Data Cleaning and Preprocessing ---\n",
    "\n",
    "# Drop the irrelevant index column\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Handle missing values by dropping rows where 'statement' is NaN\n",
    "# Since only 362 out of 53043 are missing, dropping them is generally safe.\n",
    "df.dropna(subset=['statement'], inplace=True)\n",
    "print(f\"Data shape after handling missing values: {df.shape}\")\n",
    "\n",
    "# Convert all text to string (ensures robustness)\n",
    "df['statement'] = df['statement'].astype(str)\n",
    "\n",
    "# --- 3. Label Encoding for the Target Variable (status) ---\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Fit and transform the 'status' column\n",
    "df['status_encoded'] = le.fit_transform(df['status'])\n",
    "\n",
    "# Display the mapping\n",
    "print(\"\\n--- Label Encoding Mapping ---\")\n",
    "for i, label in enumerate(le.classes_):\n",
    "    print(f\"{label}: {i}\")\n",
    "\n",
    "\n",
    "# --- 4. GPT-2 Feature Extraction Function ---\n",
    "\n",
    "def get_gpt2_embeddings(texts, model, tokenizer, batch_size=32):\n",
    "    \"\"\"Generates sentence embeddings using GPT-2 by mean pooling.\"\"\"\n",
    "\n",
    "    # Set pad token for GPT-2 (it doesn't have one by default)\n",
    "    # Using the EOS token as PAD token is a common workaround for feature extraction\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Place model in evaluation mode and move to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    # Process texts in batches to manage memory\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "        # Tokenize the batch\n",
    "        # Padding='longest' for dynamic padding, Truncation=True to prevent sequence overflow\n",
    "        # return_tensors='pt' returns PyTorch tensors\n",
    "        encoded_input = tokenizer(\n",
    "            batch_texts.tolist(),\n",
    "            padding='longest',\n",
    "            truncation=True,\n",
    "            max_length=128,  # Choose a max length based on data, 128 is common\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Move tensors to the appropriate device\n",
    "        input_ids = encoded_input['input_ids'].to(device)\n",
    "        attention_mask = encoded_input['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get the model output\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # The last hidden state contains the token embeddings\n",
    "            embeddings = output.last_hidden_state  # Shape: (Batch Size, Sequence Length, Hidden Dimension)\n",
    "\n",
    "        # --- Mean Pooling to get Sentence Vector ---\n",
    "        # Multiply embeddings by the attention mask to zero out padding tokens\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "        masked_embeddings = embeddings * mask_expanded\n",
    "\n",
    "        # Sum all non-padding tokens\n",
    "        summed_embeddings = torch.sum(masked_embeddings, 1)\n",
    "\n",
    "        # Calculate the actual number of tokens (non-padding)\n",
    "        summed_mask = torch.clamp(attention_mask.sum(1), min=1e-9)  # Avoid division by zero\n",
    "\n",
    "        # Divide sum by count to get the mean (mean pooling)\n",
    "        mean_pooled_embeddings = (summed_embeddings / summed_mask.unsqueeze(-1)).cpu().numpy()\n",
    "\n",
    "        all_embeddings.append(mean_pooled_embeddings)\n",
    "\n",
    "    return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "\n",
    "# Initialize GPT-2 components\n",
    "print(\"\\n--- Loading GPT-2 Model and Tokenizer (This may take a moment) ---\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Generate the feature vectors (X)\n",
    "X_features = get_gpt2_embeddings(df['statement'], model, tokenizer, batch_size=64)\n",
    "Y_labels = df['status_encoded'].values\n",
    "\n",
    "print(f\"\\nGenerated Feature Matrix X shape: {X_features.shape}\")\n",
    "print(f\"Generated Label Vector Y shape: {Y_labels.shape}\")\n",
    "\n",
    "# --- 5. Data Splitting and Logistic Regression ---\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_features, Y_labels, test_size=0.2, random_state=42, stratify=Y_labels\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining on {X_train.shape[0]} samples, Testing on {X_test.shape[0]} samples.\")\n",
    "\n",
    "# Initialize and train Logistic Regression model\n",
    "print(\"\\n--- Training Logistic Regression Model ---\")\n",
    "# Use a high max_iter and appropriate solver for large datasets/features\n",
    "lr_model = LogisticRegression(max_iter=500, solver='sag', multi_class='multinomial', random_state=42)\n",
    "lr_model.fit(X_train, Y_train)\n",
    "\n",
    "# --- 6. Model Evaluation ---\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred = lr_model.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Evaluation Results (Logistic Regression with GPT-2 Features) ---\")\n",
    "print(classification_report(Y_test, Y_pred, target_names=le.classes_))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
